{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[View as slides](https://nbviewer.jupyter.org/format/slides/github/lutostag/talks/blob/master/python/Benchmarks.ipynb#/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Benchmarking + Regressions</h1></center>\n",
    "  \n",
    "Greg Lutostanski\n",
    "\n",
    "[github.com/lutostag](https://github.com/lutostag)\n",
    "\n",
    "Senior Software Architect\n",
    "    \n",
    "  \n",
    "![The Mobility House](https://www.mobilityhouse.com/media/logo/default/tmh_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Give you all some tools to use for benchmarking/understand the landscape.\n",
    "\n",
    "Particularly tied to some tools we already use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Benchmarks:\n",
    "* A type of test\n",
    "* How fast something is (relative to something else)\n",
    "* Hard to get right\n",
    "    * what is measured\n",
    "    * consistency\n",
    "\n",
    "Regressions:\n",
    "* Has something gotten worse (since before -- some checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Standards:\n",
    "* [xUnit](https://en.wikipedia.org/wiki/XUnit) / jUnit [...](https://martinfowler.com/bliki/Xunit.html)\n",
    "    \n",
    "started in 1998 -- has some standard parseable xml output that looks like:\n",
    "```\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?><testsuite errors=\"0\" failures=\"0\" name=\"pytest\" skips=\"0\" tests=\"47\" time=\"3.592\">\n",
    "<testcase classname=\"marketplace.tests.test_aggregator\" file=\"marketplace/tests/test_aggregator.py\" line=\"8\" name=\"test_aggregate_empty_input\" time=\"0.0025908946990966797\">\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tools:\n",
    "* pytest\n",
    "* pytest-benchmark (for benchmarking obviously)\n",
    "    \n",
    "Other languages:\n",
    "* karma\n",
    "* karma-benchmark\n",
    "\n",
    "Basically any *good* testing framework should have a benchmark plugin -- and should write xUnit xml output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also circleci...\n",
    "\n",
    "Lets you save test artifacts -- and can read xUnit output files in trends too...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q pytest pytest-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# test.py\n",
    "import time\n",
    "\n",
    "def test_my_stuff(benchmark):\n",
    "    benchmark(time.sleep, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with open('test.py', 'w') as f:\n",
    "    f.write(\"\"\"import time\n",
    "\n",
    "def test_my_stuff(benchmark):\n",
    "    benchmark(time.sleep, 0.02)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.1, pytest-4.5.0, py-1.8.0, pluggy-0.11.0\n",
      "benchmark: 3.2.2 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/lutostag/python\n",
      "plugins: benchmark-3.2.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test.py \u001b[32m.\u001b[0m\u001b[36m                                                                [100%]\u001b[0m\n",
      "\u001b[35mWrote benchmark data in: <_io.BufferedWriter name='/tmp/bench.json'>\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[33m---------------------------------------------- benchmark: 1 tests ---------------------------------------------\u001b[0m\n",
      "Name (time in ms)         Min      Max     Mean  StdDev   Median     IQR  Outliers      OPS  Rounds  Iterations\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_my_stuff       \u001b[1m  20.0695\u001b[0m\u001b[1m  20.2069\u001b[0m\u001b[1m  20.1267\u001b[0m\u001b[1m  0.0327\u001b[0m\u001b[1m  20.1148\u001b[0m\u001b[1m  0.0386\u001b[0m      13;2\u001b[1m  49.6852\u001b[0m      50           1\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 2.25 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test.py --benchmark-json=/tmp/bench.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mComparing against benchmarks from: /tmp/bench.json\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.1, pytest-4.5.0, py-1.8.0, pluggy-0.11.0\n",
      "benchmark: 3.2.2 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/lutostag/python\n",
      "plugins: benchmark-3.2.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test.py \u001b[32m.\u001b[0m\u001b[36m                                                                [100%]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m---------------------------------------------------------------------------------------- benchmark: 2 tests ---------------------------------------------------------------------------------------\u001b[0m\n",
      "Name (time in ms)                    Min                 Max                Mean            StdDev              Median               IQR            Outliers      OPS            Rounds  Iterations\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_my_stuff (//tmp/bench)   \u001b[32m\u001b[1m   20.0695 (1.0)    \u001b[0m\u001b[32m\u001b[1m   20.2069 (1.0)    \u001b[0m\u001b[32m\u001b[1m   20.1267 (1.0)    \u001b[0m\u001b[32m\u001b[1m  0.0327 (1.0)    \u001b[0m\u001b[32m\u001b[1m   20.1148 (1.0)    \u001b[0m\u001b[32m\u001b[1m  0.0386 (1.0)    \u001b[0m      13;2\u001b[32m\u001b[1m  49.6852 (1.0)    \u001b[0m      50           1\n",
      "test_my_stuff (NOW)           \u001b[31m\u001b[1m  100.1481 (4.99)   \u001b[0m\u001b[31m\u001b[1m  100.3395 (4.97)   \u001b[0m\u001b[31m\u001b[1m  100.2054 (4.98)   \u001b[0m\u001b[31m\u001b[1m  0.0562 (1.72)   \u001b[0m\u001b[31m\u001b[1m  100.1967 (4.98)   \u001b[0m\u001b[31m\u001b[1m  0.0626 (1.62)   \u001b[0m       2;1\u001b[31m\u001b[1m   9.9795 (0.20)   \u001b[0m      10           1\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\u001b[32m\u001b[1m=========================== 1 passed in 2.23 seconds ===========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Now lets slow it down to compare...\n",
    "!sed -i test.py -e 's/0.02/0.10/g'\n",
    "!pytest test.py --benchmark-compare=/tmp/bench.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mComparing against benchmarks from: /tmp/bench.json\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.1, pytest-4.5.0, py-1.8.0, pluggy-0.11.0\n",
      "benchmark: 3.2.2 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/lutostag/python\n",
      "plugins: benchmark-3.2.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test.py \u001b[32m.\u001b[0m\u001b[36m                                                                [100%]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m---------------------------------------------------------------------------------------- benchmark: 2 tests ---------------------------------------------------------------------------------------\u001b[0m\n",
      "Name (time in ms)                    Min                 Max                Mean            StdDev              Median               IQR            Outliers      OPS            Rounds  Iterations\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_my_stuff (//tmp/bench)   \u001b[32m\u001b[1m   20.0695 (1.0)    \u001b[0m\u001b[32m\u001b[1m   20.2069 (1.0)    \u001b[0m\u001b[32m\u001b[1m   20.1267 (1.0)    \u001b[0m\u001b[32m\u001b[1m  0.0327 (1.0)    \u001b[0m\u001b[32m\u001b[1m   20.1148 (1.0)    \u001b[0m\u001b[32m\u001b[1m  0.0386 (1.0)    \u001b[0m      13;2\u001b[32m\u001b[1m  49.6852 (1.0)    \u001b[0m      50           1\n",
      "test_my_stuff (NOW)           \u001b[31m\u001b[1m  100.1663 (4.99)   \u001b[0m\u001b[31m\u001b[1m  100.4025 (4.97)   \u001b[0m\u001b[31m\u001b[1m  100.2524 (4.98)   \u001b[0m\u001b[31m\u001b[1m  0.0708 (2.17)   \u001b[0m\u001b[31m\u001b[1m  100.2336 (4.98)   \u001b[0m\u001b[31m\u001b[1m  0.0526 (1.36)   \u001b[0m       3;2\u001b[31m\u001b[1m   9.9748 (0.20)   \u001b[0m      10           1\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\n",
      "\u001b[31m\u001b[1m--------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m\u001b[1mPerformance has regressed:\n",
      "\ttest_my_stuff (//tmp/bench) - Field 'min' has failed PercentageRegressionCheck: 399.096903594 > 5.000000000\u001b[0m\n",
      "\u001b[31m\u001b[1m--------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Performance has regressed.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#we can also make it fail if it regresses too far... just add the option:\n",
    "!pytest test.py --benchmark-compare=/tmp/bench.json --benchmark-compare-fail=min:5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But how do we automate this?\n",
    "CircleCI:\n",
    "* [build artifacts](https://circleci.com/docs/2.0/artifacts/#section=jobs)\n",
    "* [API usage](https://circleci.com/docs/api/#get-authenticated)\n",
    "\n",
    "Basically store the benchmark for each run -- and compare to a baseline run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not entirely live demo\n",
    "[github](https://github.com/lutostag/benchmark-circleci)\n",
    "[circleci](https://circleci.com/build-insights/gh/lutostag/benchmark-circleci/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Questions:\n",
    "* So now you have some tools to benchmark, what does this make sense for?\n",
    "* What would you actually want to benchmark?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "rise": {
   "theme": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
